{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json, os\n",
    "from datasets import Dataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"jhu-clsp/mmBERT-base\", use_fast=True)\n",
    "\n",
    "def build_sentence_pair_dataset(folder):\n",
    "    \"\"\"\n",
    "    Reads all txt + json truth files and turns them into \n",
    "    sentence-pair samples with binary labels.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    txt_files = sorted([f for f in os.listdir(folder) if f.endswith(\".txt\")])\n",
    "\n",
    "    for fname in txt_files:\n",
    "        problem_id = fname[:-4]\n",
    "        with open(os.path.join(folder, fname), \"r\", encoding=\"utf8\") as f:\n",
    "            sentences = [s.strip() for s in f.readlines() if s.strip()]\n",
    "\n",
    "        truth = json.load(open(os.path.join(folder, \"truth-\" + problem_id + \".json\")))\n",
    "        changes = truth[\"changes\"]  # length = len(sentences) - 1\n",
    "\n",
    "        # For each boundary, create: (S_i, S_(i+1)), label\n",
    "        for i in range(len(changes)):\n",
    "            pair_text = (sentences[i], sentences[i+1])\n",
    "            label = changes[i]\n",
    "\n",
    "            texts.append(pair_text)\n",
    "            labels.append(label)\n",
    "\n",
    "    return Dataset.from_dict({\"text_pair\": texts, \"label\": labels})\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    sent1 = [t[0] for t in batch[\"text_pair\"]]\n",
    "    sent2 = [t[1] for t in batch[\"text_pair\"]]\n",
    "\n",
    "    return tokenizer(\n",
    "        sent1,\n",
    "        sent2,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_easy  = build_sentence_pair_dataset(\"Data/easy/train\")\n",
    "train_med   = build_sentence_pair_dataset(\"Data/medium/train\")\n",
    "train_hard  = build_sentence_pair_dataset(\"Data/hard/train\")\n",
    "\n",
    "valid_easy  = build_sentence_pair_dataset(\"Data/easy/validation\")\n",
    "valid_med   = build_sentence_pair_dataset(\"Data/medium/validation\")\n",
    "valid_hard  = build_sentence_pair_dataset(\"Data/hard/validation\")\n",
    "\n",
    "train_dataset = concatenate_datasets([train_easy, train_med, train_hard])\n",
    "eval_dataset  = concatenate_datasets([valid_easy, valid_med, valid_hard])\n",
    "\n",
    "train_dataset_tokenized = train_dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_pair\"]\n",
    ")\n",
    "\n",
    "eval_dataset_tokenized = eval_dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text_pair\"]\n",
    ")\n",
    "\n",
    "train_dataset_tokenized.save_to_disk(\"Data/tokenized_train_dataset\")\n",
    "eval_dataset_tokenized.save_to_disk(\"Data/tokenized_eval_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1da279",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset_tokenized[0])\n",
    "print(eval_dataset_tokenized[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
